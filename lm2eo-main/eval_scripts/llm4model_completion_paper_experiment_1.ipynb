{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Evaluation of the LLM 4 Model Completion Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "The first experiments aims at answering the research question \"To what extend can pre-trained language models and few-shot learning be used for the completion of software models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_TO_RESULTS = './../../model_completion_dataset/revision/results/results_iso_check.csv'\n",
    "#PATH_TO_RESULTS = './../../model_completion_dataset/SMO/results/few_shot_samples/stats_combined_new.csv'\n",
    "PATH_TO_RESULTS = './../../model_completion_dataset/Synthetic/results/results_iso_check.csv'\n",
    "PATH_TO_RESULTS_RANDOM = './../../model_completion_dataset/SMO/results/few_shot_samples/experiment_retrieval_comparison/stats.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all libraries. Make sure to have them installed (e.g., via pip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "from scipy import stats # for statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the results file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(PATH_TO_RESULTS)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some datasets, we have chosen a subset of samples for model completion and manual evaluation. We select only these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'in_sample' in results_df.columns:\n",
    "    results_df = results_df[results_df['in_sample'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 -- Part 1\n",
    "As a first step, we want to understand how correct the completions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to know how many completions are\n",
    " - correct according to the serialization format\n",
    " - structurally correct (the generated edge and the ground-truth edge have the same source and target node)\n",
    " - change type correct (structurally correct and the nodes and edges have the correct change type, i.e., preserved, removed, added, or change attribute)\n",
    " - type correct (i.e., change type correct and the correct reference type and correct classes of source and target nodes)\n",
    " - semantically correct (i.e., the meaning of the completion is the same as for the ground truth)\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(results_df)\n",
    "correct_format = len(results_df[results_df['correct_format'] == True])\n",
    "correct_structure = len(results_df[results_df['structural_isomorphic_completion'] == True])\n",
    "correct_change_structure = len(results_df[results_df['change_type_isomorphic_completion'] == True])\n",
    "correct_type = len(results_df[results_df['type_isomorphic_completion'] == True])\n",
    "\n",
    "result_dict = {'total_count': total_count,\n",
    "               'correct_format': correct_format,\n",
    "               'correct_structure': correct_structure,\n",
    "               'correct_change_structure': correct_change_structure,\n",
    "               'correct_type': correct_type}\n",
    "\n",
    "# We only check the semantic correctness for some datasets (because it is quite a lot of manual effort and does even not make sense for the synthetic dataset)\n",
    "if 'correctness' in results_df.columns:\n",
    "    conceivable_semantic = len(results_df[results_df['correctness'] >= 1.0])\n",
    "    correct_semantic = len(results_df[results_df['correctness'] == 2.0])\n",
    "    result_dict['conceivable_semantic'] = conceivable_semantic\n",
    "    result_dict['correct_semantic'] = correct_semantic\n",
    "\n",
    "correctness_eval_df = pd.DataFrame(columns=['property', 'count', 'relative'])\n",
    "correctness_eval_df['property'] = result_dict.keys()\n",
    "correctness_eval_df['count'] = result_dict.values()\n",
    "correctness_eval_df['relative'] = correctness_eval_df['count']/total_count\n",
    "\n",
    "\n",
    "\n",
    "correctness_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Part 2\n",
    "As a next step, we want to understand the relationship between the number of examples provided and the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'correctness' in results_df.columns:\n",
    "    results_df[['few_shot_count', 'correctness']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can check if the average number of few-shot samples for the correct ones is above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distribution = results_df['few_shot_count']\n",
    "if 'correctness' in results_df.columns:\n",
    "    few_shot_count_correct = results_df[results_df['correctness'] == 2.0]['few_shot_count']\n",
    "    print(stats.mannwhitneyu(original_distribution, few_shot_count_correct, alternative=\"greater\"))\n",
    "few_shot_count_type_correct = results_df[results_df['type_isomorphic_completion'] == True]['few_shot_count']\n",
    "plt.hist(original_distribution, bins=12, color='red')\n",
    "plt.hist(few_shot_count_type_correct, bins=12, color='green')\n",
    "plt.show()\n",
    "\n",
    "print(stats.mannwhitneyu(original_distribution, few_shot_count_type_correct, alternative=\"greater\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other SE tasks (e.g., Code Summarization Ahmed, Toufique, and Premkumar Devanbu. \"Few-shot training LLMs for project-specific code-summarization.\" Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.) it has been found that 1-shot learning doesn't perform well. We therefore compare 1-shot learning results to the overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(results_df)\n",
    "one_shot = results_df[results_df['few_shot_count'] == 1]\n",
    "one_shot_total = len(one_shot)\n",
    "if 'correctness' in results_df.columns:\n",
    "      correct_1_shot_count = len(one_shot[one_shot['correctness'] == 2])\n",
    "      correct_all_count = len(results_df[results_df['correctness'] == 2])\n",
    "      \n",
    "      correctness_all = results_df[results_df['few_shot_count'] > 1]['correctness']\n",
    "      #avg_correctness_1_shot = correctness_1_shot.mean()\n",
    "      #avg_correctness_all = correctness_all.mean()\n",
    "\n",
    "      print(f\"Mean correctness for 1-shot: {correct_1_shot_count/one_shot_total}\\n\" +\n",
    "            f\"Mean correctness all: {correct_all_count/total}\")\n",
    "\n",
    "      print(stats.mannwhitneyu(one_shot['correctness'], correctness_all, alternative='less'))\n",
    "      \n",
    "all = results_df['type_isomorphic_completion']\n",
    "p_correct_type = len(all[all==True])/len(all)\n",
    "\n",
    "correctness_1_shot_type = results_df[results_df['few_shot_count'] == 1]['type_isomorphic_completion']\n",
    "p_1_shot = len(correctness_1_shot_type[correctness_1_shot_type==True]) / len(correctness_1_shot_type)\n",
    "\n",
    "print(f\"Type correct all: {p_correct_type}. Type correct 1_shot: {p_1_shot}\")\n",
    "\n",
    "print(stats.binomtest(len(correctness_1_shot_type[correctness_1_shot_type==True]), len(correctness_1_shot_type), p=p_correct_type, alternative='less'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Part 3\n",
    "Now, we want to understand how the correctness depends on the presence of a similar example. We record the presence of a similar example only manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correct = results_df['correctness']\n",
    "similar_correct = results_df[results_df['similar_few_shot'] == True]['correctness']\n",
    "print(stats.mannwhitneyu(all_correct, similar_correct, alternative='less'))\n",
    "\n",
    "all_correct = results_df[results_df['correctness'] == 2]\n",
    "similar_correct = all_correct[all_correct['similar_few_shot'] == True]\n",
    "\n",
    "all_correct_count = len(all_correct)\n",
    "all_count = len(results_df)\n",
    "similar_correct_count = len(similar_correct)\n",
    "all_similar_count = len(results_df[results_df['similar_few_shot'] == True])\n",
    "\n",
    "stats.binomtest(similar_correct_count, all_similar_count, all_correct_count/all_count, alternative='greater')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Part 4\n",
    "Last but not least, we compare the results of random retrieval vs. semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_random = pd.read_csv(PATH_TO_RESULTS_RANDOM)\n",
    "if 'in_sample' in results_df_random.columns:\n",
    "    results_df_random = results_df_random[results_df_random['in_sample'] == 'yes']\n",
    "results_df_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_correct = len(results_df[results_df['correctness'] == 2])/len(results_df)\n",
    "\n",
    "\n",
    "\n",
    "correct_format = results_df['correct_format']\n",
    "p_correct_format = len(correct_format[correct_format==True])/len(correct_format)\n",
    "\n",
    "\n",
    "correct_structure = results_df['structural_isomorphic_completion']\n",
    "correct_change_structure = results_df['change_type_isomorphic_completion']\n",
    "correct_type = results_df['type_isomorphic_completion']\n",
    "p_correct_type = len(correct_type[correct_type==True])/len(correct_type)\n",
    "\n",
    "correct_format_random = results_df_random['correct_format']\n",
    "correct_structure_random = results_df_random['structural_isomorphic_completion']\n",
    "correct_change_structure_random = results_df_random['change_type_isomorphic_completion']\n",
    "correct_type_random = results_df_random['type_isomorphic_completion']\n",
    "\n",
    "print(stats.binomtest(len(correct_type_random[correct_type_random==True]), len(correct_type_random), p=p_correct, alternative='less'))\n",
    "print(stats.binomtest(len(correct_type_random[correct_type_random==True]), len(correct_type_random), p=p_correct_type, alternative='less'))\n",
    "print(stats.binomtest(len(correct_format_random[correct_format_random==True]), len(correct_format_random), p=p_correct_format, alternative='less'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_miner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
