# Note that doing the analysis for all repositories can be quite costly, we selected specific models
# Ada, 6 epochs, D=10, EO=31, p=0.0
data_ada = read.csv("./../results_completion_ada_6_D10E31p0.csv", header = TRUE, sep=",")
# Curie, 6 epochs, D=20, EO=51, p=0.0
data_curie = read.csv("./../results_completion_curie_6_D20E51p0.csv", header = TRUE, sep=";")
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
eo_data_combined <- rbind(stats_ada, stats_curie, stats_davinci)
View(eo_data_combined)
summary(eo_data_combined$Average_Token_Acc)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
if (!require("RColorBrewer")) {
install.packages("RColorBrewer")
library(RColorBrewer)
}
# Load results csv for all evaluated models
# Note that doing the analysis for all repositories can be quite costly, we selected specific models
# Ada, 6 epochs, D=10, EO=31, p=0.0
data_ada = read.csv("./../results_completion_ada_6_D10E31p0.csv", header = TRUE, sep=",")
# Curie, 6 epochs, D=20, EO=51, p=0.0
data_curie = read.csv("./../results_completion_curie_6_D20E51p0.csv", header = TRUE, sep=";")
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
eo_data_combined <- rbind(stats_ada, stats_curie, stats_davinci)
# Combine all results
data_combined <- rbind(data_ada, data_curie)
# Load results csv for all evaluated models
# Note that doing the analysis for all repositories can be quite costly, we selected specific models
# Ada, 6 epochs, D=10, EO=31, p=0.0
data_ada = read.csv("./../results_completion_ada_6_D10E31p0.csv", header = TRUE, sep=",")
# Curie, 6 epochs, D=20, EO=51, p=0.0
data_curie = read.csv("./../results_completion_curie_6_D20E51p0.csv", header = TRUE, sep=";")
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
eo_data_combined <- rbind(stats_ada, stats_curie, stats_davinci)
# Combine all results
data_combined <- rbind(data_ada, data_curie, fill = TRUE)
data_combined <- rbind(data_ada, data_curie, fill = TRUE)
data_combined <- rbindlist(data_ada, data_curie, fill = TRUE)
}
require(data.table) ## 1.9.3 commit 1267
data_combined <- rbindlist(data_ada, data_curie, fill = TRUE)
data_combined <- rbind(data_ada, data_curie, fill = TRUE)
data_combined <- bind_rows(data_ada, data_curie)
ODO remove this again
data <- data_combined
########################################################################################
# Utils
plot_cor <- function(cor_mat){
col<- colorRampPalette(c("blue", "white", "red"))(20)
corrplot(cor_mat, addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
type="upper", order="hclust", diag=FALSE, col=col)
}
########################################################################################
########################### Correlations ################################################
colnames(data)
#compute correlations
stats_temp <- data %>% select(missing_edges, total_edges, nb_completion_candidates, completion_best_rank_score, completion_best_eval_score)
cors <- cor(stats_temp, method="spearman")
plot_cor(cors)
View(data)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
if (!require("RColorBrewer")) {
install.packages("RColorBrewer")
library(RColorBrewer)
}
require(data.table) ## 1.9.3 commit 1267
# Load results csv for all evaluated models
# Note that doing the analysis for all repositories can be quite costly, we selected specific models
# Ada, 6 epochs, D=10, EO=31, p=0.0
data_ada = read.csv("./../results_completion_ada_6_D10E31p0.csv", header = TRUE, sep=",")
# Curie, 6 epochs, D=20, EO=51, p=0.0
data_curie = read.csv("./../results_completion_curie_6_D20E51p0.csv", header = TRUE, sep=";")
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
eo_data_combined <- rbind(stats_ada, stats_curie, stats_davinci)
# Combine all results
data_combined <- bind_rows(data_ada, data_curie)
data <- data_combined
########################################################################################
# Utils
plot_cor <- function(cor_mat){
col<- colorRampPalette(c("blue", "white", "red"))(20)
corrplot(cor_mat, addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
type="upper", order="hclust", diag=FALSE, col=col)
}
########################################################################################
########################### Correlations ################################################
colnames(data)
#compute correlations
stats_temp <- data %>% select(missing_edges, total_edges, nb_completion_candidates, completion_best_rank_score, completion_best_eval_score)
cors <- cor(stats_temp, method="spearman")
plot_cor(cors)
View(data)
View(ds_curie)
View(data_curie)
View(data_ada)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
if (!require("RColorBrewer")) {
install.packages("RColorBrewer")
library(RColorBrewer)
}
require(data.table) ## 1.9.3 commit 1267
# Load results csv for all evaluated models
# Note that doing the analysis for all repositories can be quite costly, we selected specific models
# Ada, 6 epochs, D=10, EO=31, p=0.0
data_ada = read.csv("./../results_completion_ada_6_D10E31p0.csv", header = TRUE, sep=";")
# Curie, 6 epochs, D=20, EO=51, p=0.0
data_curie = read.csv("./../results_completion_curie_6_D20E51p0.csv", header = TRUE, sep=";")
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
eo_data_combined <- rbind(stats_ada, stats_curie, stats_davinci)
# Combine all results
data_combined <- bind_rows(data_ada, data_curie)
# TODO remove this again
#data$missing_edges <- data$total_edges - data$prompt_edges
#data$best_rank <- ifelse(data$completion_best_rank_result==data$completion_best_eval_result,0,data$nb_completion_candidates)
#write_excel_csv2(data, "./../results_completion_ada_6_D10E31p0.csv", append = FALSE, col_names = TRUE, )
# Select dataset
data <- data_combined
View(data)
########################################################################################
# Utils
plot_cor <- function(cor_mat){
col<- colorRampPalette(c("blue", "white", "red"))(20)
corrplot(cor_mat, addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
type="upper", order="hclust", diag=FALSE, col=col)
}
########################################################################################
########################### Correlations ################################################
colnames(data)
#compute correlations
stats_temp <- data %>% select(missing_edges, total_edges, nb_completion_candidates, completion_best_rank_score, completion_best_eval_score)
cors <- cor(stats_temp, method="spearman")
plot_cor(cors)
corstarsl(stats_temp, type=c("spearman"))
########################################################################################
########################## Basic stats ##############################################
#Average number of missing lines, edges in ground truth,... (i.e., difficulty of the completion assessment)
average_missing = sum(data$missing_edges)/count(data)
average_edges = sum(data$total_edges)/count(data)
average_nb_candidates = sum(data$nb_completion_candidates)/count(data)
# Correct completions
correct_data = data %>% filter(completion_best_eval_result == "ISOMORPHIC")
average_correct = count(correct_data)/count(data)
average_rank = sum(correct_data$best_rank)/count(correct_data)
# As a sanity check if our dataset generation works, we plot the ratio of prompt vs. total edges
prompt_vs_total = data$prompt_edges / data$total_edges
# Our generation should yield a 3-modal distribution, we use Ameijeiras-Alonso et al. (2019) excess mass test to verify 3-modality
install.packages('multimode')
library(multimode)
modetest(prompt_vs_total, mod0=2, method="ACR")
hist(prompt_vs_total)
install.packages("multimode")
########################## Basic stats ##############################################
#Average number of missing lines, edges in ground truth,... (i.e., difficulty of the completion assessment)
average_missing = sum(data$missing_edges)/count(data)
average_edges = sum(data$total_edges)/count(data)
average_nb_candidates = sum(data$nb_completion_candidates)/count(data)
# Correct completions
correct_data = data %>% filter(completion_best_eval_result == "ISOMORPHIC")
average_correct = count(correct_data)/count(data)
average_rank = sum(correct_data$best_rank)/count(correct_data)
View(average_nb_candidates)
View(average_nb_candidates)
View(average_nb_candidates)
View(average_correct)
View(average_correct)
View(average_correct_ada)
View(average_correct)
View(average_correct)
View(average_correct)
View(average_correct)
View(average_correct)
View(average_correct)
View(average_correct)
View(average_correct)
View(average_missing)
View(average_missing)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
if (!require("RColorBrewer")) {
install.packages("RColorBrewer")
library(RColorBrewer)
}
###  table for Latex
library(xtable)
############################# Utils ######################################################
corstarsl <- function(x, type=c('pearson')){
require(Hmisc)
x <- as.matrix(x)
R <- rcorr(x, type=type)$r
p <- rcorr(x, type=type)$P
## define notions for significance levels; spacing is important.
mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "* ", " ")))
## trunctuate the matrix that holds the correlations to two decimal
R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
diag(Rnew) <- paste(diag(R), " ", sep="")
rownames(Rnew) <- colnames(x)
colnames(Rnew) <- paste(colnames(x), "", sep="")
## remove upper triangle
Rnew <- as.matrix(Rnew)
Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
## remove last column and return the matrix (which is now a data frame)
Rnew <- cbind(Rnew[1:length(Rnew)-1])
return(Rnew)
}
###########################################################################################
####################### Load dataset #####################################################
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
stats <- rbind(stats_ada, stats_curie)
stats_inc_davinci <- rbind(stats_ada, stats_curie, stats_davinci)
######################### Adding derived value #############################################
## Add numeric correspondance for base model
stats$Base_Model_O <- stats$Base_Model
stats["Base_Model_O"][stats["Base_Model"] == "ada"] <- 0
stats["Base_Model_O"][stats["Base_Model"] == "curie"] <- 1
stats["Base_Model_O"][stats["Base_Model"] == "davinci"] <- 2
stats$Base_Model_O <- as.numeric(stats$Base_Model_O )
## Add numeric correspondance for base model
stats_inc_davinci$Base_Model_O <- stats_inc_davinci$Base_Model
stats_inc_davinci["Base_Model_O"][stats_inc_davinci["Base_Model"] == "ada"] <- 0
stats_inc_davinci["Base_Model_O"][stats_inc_davinci["Base_Model"] == "curie"] <- 1
stats_inc_davinci["Base_Model_O"][stats_inc_davinci["Base_Model"] == "davinci"] <- 2
stats_inc_davinci$Base_Model_O <- as.numeric(stats_inc_davinci$Base_Model_O )
plot_cor <- function(cor_mat){
col<- colorRampPalette(c("blue", "white", "red"))(20)
corrplot(cor_mat, addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
type="upper", order="hclust", diag=FALSE, col=col)
}
###########################################################################################
# The correlations for ada and curie
stats_temp <- stats %>% select(Base_Model_O, Epochs, Diffs, EOs, Pertubation, Number_Tokens,  Epochs, faulty_graphs, faulty_mm, token_count)
cors <- cor(stats_temp, method="pearson")
plot_cor(cors)
# Some graphs
pairs(stats_temp[, c("faulty_graphs", "Number_Tokens", "Pertubation", "Epochs")])
# The correlations for all three models
stats_temp_all <- stats_inc_davinci %>% filter(Pertubation==1.0) %>% select(Base_Model_O, Epochs, Diffs, EOs, Number_Tokens,  Epochs, faulty_graphs, faulty_mm, token_count)
cors <- cor(stats_temp_all, method="pearson")
plot_cor(cors)
# Check for significant dependency on the model
cor.test(stats_temp$faulty_graphs, stats_temp$Base_Model_O)
cor.test(stats_temp_all$faulty_graphs, stats_temp_all$Base_Model_O)
# Check for significant dependency on the Number of Tokens in the dataset
cor.test(stats_temp$faulty_graphs, stats_temp$Number_Tokens)
cor.test(stats_temp_all$faulty_graphs, stats_temp_all$Number_Tokens)
# Check for significant dependency on the Number of Epochs
cor.test(stats_temp$faulty_graphs, stats_temp$Epochs)
cor.test(stats_temp_all$faulty_graphs, stats_temp_all$Epochs)
# Overall percentage of all correct serializations
data_values = count(stats_inc_davinci)
mean(stats_inc_davinci$faulty_graphs)
min(stats_inc_davinci$faulty_graphs)
max(stats_inc_davinci$faulty_graphs)
zero_faulty = count(stats_inc_davinci  %>% filter(faulty_graphs==0))
zero_faulty_all = count(stats_inc_davinci  %>% filter(faulty_graphs==0 & faulty_mm==0))
zero_faulty_relativ = zero_faulty/data_values
zero_faulty_all_relativ = zero_faulty_all/data_values
hist(stats_inc_davinci$faulty_graphs, breaks = 10)
corstarsl(stats_temp)
xtable(corstarsl(stats_temp)) #Latex code
corstarsl(stats_temp_all)
xtable(corstarsl(stats_temp_all)) #Latex code
corstarsl(stats_temp, method="spearman")
corstarsl <- function(x, type=c('pearson')){
require(Hmisc)
x <- as.matrix(x)
R <- rcorr(x, type=type)$r
p <- rcorr(x, type=type)$P
## define notions for significance levels; spacing is important.
mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "* ", " ")))
## trunctuate the matrix that holds the correlations to two decimal
R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
diag(Rnew) <- paste(diag(R), " ", sep="")
rownames(Rnew) <- colnames(x)
colnames(Rnew) <- paste(colnames(x), "", sep="")
## remove upper triangle
Rnew <- as.matrix(Rnew)
Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
## remove last column and return the matrix (which is now a data frame)
Rnew <- cbind(Rnew[1:length(Rnew)-1])
return(Rnew)
}
corstarsl(stats_temp, method="spearman")
corstarsl(stats_temp, type="spearman")
corstarsl(stats_temp, type="pearson")
corstarsl(stats_temp, type="spearman")
corstarsl(stats_temp, type="pearson")
plot.new()
plot( stats_temp %>% select(faulty_mm, Pertubation ), col="red")
par(new=TRUE)
plot.new()
# nb of eos vs. nb of components per diff
avg_faulty_mm <- aggregate(faulty_mm  ~ Pertubation, data=stats_temp, mean, na.rm=TRUE)
plot( stats_temp %>% select(Pertubation, faulty_mm ), col="red")
lines(avg_faulty_mm, col="black")
par(new=TRUE)
plot.new()
# nb of eos vs. nb of components per diff
avg_faulty_mm <- aggregate(faulty_mm  ~ Pertubation, data=stats_temp, mean, na.rm=TRUE)
plot( stats_temp %>% select(Pertubation, faulty_mm ), col="red")
lines(avg_faulty_mm, col="black")
plot.new()
# nb of eos vs. nb of components per diff
avg_faulty_mm <- aggregate(faulty_mm  ~ Pertubation, data=stats_temp, mean, na.rm=TRUE)
plot( stats_temp %>% select(Pertubation, faulty_mm ), col="red")
lines(avg_faulty_mm, col="black")
corstarsl(stats_temp, type="pearson")
corstarsl(stats_temp, type="spearman")
corstarsl(stats_temp_all)
corstarsl(stats_temp_all, type="spearman")
pairs(stats_temp[, c("faulty_graphs", "Number_Tokens", "Pertubation", "Epochs")])
mean(stats_inc_davinci$faulty_mm)
min(stats_inc_davinci$faulty_mm)
max(stats_inc_davinci$faulty_mm)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
if (!require("RColorBrewer")) {
install.packages("RColorBrewer")
library(RColorBrewer)
}
###  table for Latex
library(xtable)
#For the second experiment, replace experiment1 by experiment2. Change result_columns to: result_columns <- c("score", "score_2")
stats_ada <- read.csv("./../all_results_ada.csv", header=TRUE, sep=";")
stats_curie <- read.csv("./../all_results_curie.csv", header=TRUE, sep=";")
stats_davinci <- read.csv("./../all_results_davinci.csv", header=TRUE, sep=";")
# Combine all results
stats <- rbind(stats_ada, stats_curie, stats_davinci)
result_columns_factorial <- c("Pattern_Correct_1_Rank_factorial", "Pattern_Correct_2_Rank_factorial", "Pattern_Correct_3_Rank_factorial")
result_columns_probability <- c("Pattern_Correct_1_Rank_probability", "Pattern_Correct_2_Rank_probability", "Pattern_Correct_3_Rank_probability")
result_columns_edges_scaled <- c("Pattern_Correct_1_Rank_edges_scaled", "Pattern_Correct_2_Rank_edges_scaled", "Pattern_Correct_3_Rank_edges_scaled")
result_columns_compression <- c("Pattern_Correct_1_Rank_compression", "Pattern_Correct_2_Rank_compression", "Pattern_Correct_3_Rank_compression")
result_columns <- result_columns_compression
#
###################################### Compare the rank of Pattern 1 for models ############################
#stats_correct_1 <- stats %>% select(Id, Pattern_Correct_1_Rank_factorial,Pattern_Correct_2_Rank_factorial, Base_Model, Epochs)
#stats_correct_1 <- pivot_wider(stats_correct_1, names_from = c(Base_Model, Epochs), values_from = c(Pattern_Correct_1_Rank_factorial, Pattern_Correct_2_Rank_factorial))
#mat_temp <-column_to_rownames(stats_correct_1, "Id")
#minusOneToMinusHundred <- function(c) {if (c == -1){
#  c = 50
#}
#  return(c);
#}
#data_matrix <-as.matrix(mat_temp)
#data_matrix <- apply(data_matrix, 1:2, minusOneToMinusHundred)
#heatmap(data_matrix, Rowv = NA, Colv = NA,  scale = "none", margins=c(20,10))
###################################### Some Method definitions #############################################
### Correlation table for Latex
library(xtable)
corstarsl <- function(x, type=c('pearson')){
require(Hmisc)
x <- as.matrix(x)
R <- rcorr(x, type=type)$r
p <- rcorr(x, type=type)$P
## define notions for significance levels; spacing is important.
mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "* ", " ")))
## trunctuate the matrix that holds the correlations to two decimal
R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
diag(Rnew) <- paste(diag(R), " ", sep="")
rownames(Rnew) <- colnames(x)
colnames(Rnew) <- paste(colnames(x), "", sep="")
## remove upper triangle
Rnew <- as.matrix(Rnew)
Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
## remove last column and return the matrix (which is now a data frame)
Rnew <- cbind(Rnew[1:length(Rnew)-1])
return(Rnew)
}
# Given a list of ranks for the true documents, this function returns the average precision metric
library(comprehenr)
average_precision <- function(rank_list, k){
relevant_count = length(rank_list)
# Since we are using -1 for "not found" we have to get rid of these first
rank_list <- rank_list[rank_list != -1]
rank_list <- rank_list[rank_list <= k]
if (length(rank_list) == 0){
return(0.0);
}
# then we sort and compute the precisions
rank_list <- rank_list[sort.list(rank_list)]
precs = to_vec(for(i in 1:length(rank_list)) i/rank_list[i])
1 / relevant_count * sum(precs)
}
map_score <- function(rank_lists, k){
mean(apply(rank_lists, 1, average_precision, k=k))
}
recall_at_k <- function(rank_list, k){
relevant_count = length(rank_list)
# Since we are using -1 for "not found" we have to get rid of these first
rank_list <- rank_list[rank_list != -1]
rank_list <- rank_list[rank_list <= k]
if (length(rank_list) == 0){
return(0.0);
}
length(rank_list) / relevant_count
}
mean_recall_at_k <- function(rank_lists, k){
mean(apply(rank_lists, 1, recall_at_k, k=k))
}
############################ Add some derived values ###############################################
#stats$score[is.na(stats_topn$score)] <- -1
#stats$score_2[is.na(stats_topn$score_2)] <- -1
# relative threshold
#stats <- stats %>% mutate(threshold_relative = support_threshold / cnt_components)
# Selected metric
positions = stats %>% select(result_columns)
stats["average_precision"] <- apply(positions, 1, average_precision, k=max(positions))
# Just the number of correctly identified operations (independent of metric)
stats["count_correct"] <- apply(positions,1,function(x) {sum(x != -1)})
# Factorial metric
positions = stats %>% select(result_columns_factorial)
stats["average_precision_factorial"] <- apply(positions, 1, average_precision, k=max(positions))
# Edges scaled metric
positions = stats %>% select(result_columns_edges_scaled)
stats["average_precision_edges_scaled"] <- apply(positions, 1, average_precision, k=max(positions))
# Probability metric
positions = stats %>% select(result_columns_probability)
stats["average_precision_probability"] <- apply(positions, 1, average_precision, k=max(positions))
# Compression metric
positions = stats %>% select(result_columns_compression)
stats["average_precision_compression"] <- apply(positions, 1, average_precision, k=max(positions))
## Add numeric correspondance for base model
stats$Base_Model_O <- stats$Base_Model
stats["Base_Model_O"][stats["Base_Model"] == "ada"] <- 0
stats["Base_Model_O"][stats["Base_Model"] == "curie"] <- 1
stats["Base_Model_O"][stats["Base_Model"] == "davinci"] <- 2
stats$Base_Model_O <- as.numeric(stats$Base_Model_O )
stats_all <- stats
stats_inc_davinci <- stats %>% filter(Pertubation == 1.0)
stats <- stats %>% filter("Base_Model" != "davinci")
###################################
##################################### MAP score /MR@k #############################
##################
compute_MAP <- function(result_list, result_columns, min_k=3){
result_columns <- c(result_columns)
max_rank = max(result_list %>% select(result_columns))
# MAP
map1 = map_score(result_list %>% select(result_columns), min_k)
map5 = map_score(result_list %>% select(result_columns), 5)
map10 = map_score(result_list %>% select(result_columns), 10)
mapinfty = map_score(result_list %>% select(result_columns), max_rank)
map_matrix = matrix(c(map1, map5, map10, mapinfty), nrow=4, ncol=1)
rownames(map_matrix) <- c("MAP@3", "MAP@5", "MAP@10", "MAP@Infty")
return(map_matrix)
}
################
# We use -1 for it does not appear at all
rank_topn <- stats_all
#result_columns_factorial <- c("Pattern_Correct_1_Rank_factorial", "Pattern_Correct_2_Rank_factorial", "Pattern_Correct_3_Rank_factorial")
#result_columns_probability <- c("Pattern_Correct_1_Rank_probability", "Pattern_Correct_2_Rank_probability", "Pattern_Correct_3_Rank_probability")
#result_columns_edges_scaled <- c("Pattern_Correct_1_Rank_edges_scaled", "Pattern_Correct_2_Rank_edges_scaled", "Pattern_Correct_3_Rank_edges_scaled")
#result_columns_compression <- c("Pattern_Correct_1_Rank_compression", "Pattern_Correct_2_Rank_compression", "Pattern_Correct_3_Rank_compression")
result_columns <- matrix(c(result_columns_compression, result_columns_factorial, result_columns_probability, result_columns_edges_scaled), nrow=3, ncol=4)
colnames(result_columns) <- c("Compression", "Factorial", "Probability", "Edges Scaled")
map_scores = apply(result_columns, 2, compute_MAP, result_list=rank_topn)
rownames(map_scores) <- c("MAP@3", "MAP@5", "MAP@10", "MAP@Infty")
xtable(map_scores)
# TODO also move this to extra method
#MR@k
mean_recall_at_k(rank_topn %>% select(result_columns), 1)
ap_matrix = stats %>% select(average_precision_factorial, average_precision_probability, average_precision_edges_scaled, average_precision_compression)
corstarsl(ap_matrix)
corstarsl(ap_matrix, type="Spearman")
corstarsl(ap_matrix, type="spearman")
View(data_combined)
