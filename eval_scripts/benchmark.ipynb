{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "RESULTS_DIR = './../datasets_reduced/revision/results/'\n",
    "\n",
    "RESULTS_CHAABEN = 'results_iso_check_with_info_baseline_powerset_withanchor_likepaperbutwithoutanchors.csv'\n",
    "RESULTS_RAMC = 'results_with_comparable_info.csv' \n",
    "RESULTS_RANDOM_RAC = 'dataset_random_baseline_with_completions.csv'\n",
    "OUTPUT_FILE = 'benchmark_results.csv' #'results_with_comparable_info.csv'\n",
    "\n",
    "# cd with jupyter into the directory with the results\n",
    "os.chdir(RESULTS_DIR)\n",
    "\n",
    "\n",
    "DATASET_RESULTS_FILES = [RESULTS_RANDOM_RAC, RESULTS_RAMC, RESULTS_CHAABEN]\n",
    "dataset_name = ['random', 'ramc', 'chaaben']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Completion Benchmark\n",
    "## Some words on the comparison with Chaaben et al.\n",
    "As elaborated on in the paper, benchmarking the task of model completion is sophisticated. Some approaches perform their evaluation using concepts that are not specific to the task of model completion but rather to the approach.\n",
    "Some specifically take aspects of a particular domain into account and do not allow to benchmark against (e.g., because also the dataset is not published and their approach can not be applied to other domains).\n",
    "We investigated an enormous amount of time to understand these difficulties. Eventually, and as advised by the reviewers, we decided to benchmark our approach (and the LLM based approach with random retrieval) with Chaaben et al.'s approach.\n",
    "Their approach clearly is the closest approach to ours. Anyway, we still had to make several design decisions to be able to make this comparison. These are:\n",
    "\n",
    "- On their data or on our data? => We applied the approach on our data, since we did not find exactly on which modelset examples their approach has been evaluated. Furthermore, Modelset does not include histories/revisions.\n",
    "- On which domain models? => We think that a proper level where we can compare several approaches to model completion would be to move to a meta-meta level (following the MOF). This allows us to compare Chaaben et al.'s approach that works for a subset of UML class diagrams and activity diagrams against our approach that works directly on the abstract syntac graph of the models. By interpreting the abstract syntax of our models as class diagrams, we were able to compare (part of) their approach to ours. Having said this: We definitely agree, that domain-specific improvements are necessary and meaningful. Adapting an approach to a specific domain always makes sense. Anyway, for the concrete use case of model completion, a domain-agnostic comparison of approaches should be a goal of the community. For a domain-specific approach it is then the duty of this approach to show improvement over the domain-agnostic baseline, not the otherway round.\n",
    "In our concrete case, we performed the comparison on our sample of the Revision dataset. This implies that we could not compare against the Activity Diagram implementation. In our comparison, we did focus on concept recommendation and association recommendation. Attribute recommendation in class diagrams is quite comparable to concept recommendation. Indeed, in ECore deciding between a reference to another concept or an attribute of an EClass is more like a design choice and both are considered to be a EStructuralFeature.\n",
    "- How to implement/adapt their approach? => We looked into the code of the paper and also reused some of their implementation. Some implementation details in the current version of their code were not consistent to the description in the paper. \n",
    "We decided to align more to the description in the paper, while making the necessary adaption to work on our code. \n",
    "- How to map classes in a class diagram to the Ecore meta-level? => Revision is an Ecore dataset. In our dataset, we had already classified the changes and the changes that are of interest for the recommendation of new concepts, corresponded to a class of changes we called *Add_node* in our samples. We selected these changes (which left us with 51 samples for the revision dataset, which might seem a small number. Anyway, we performed significance test for the results and got significant results for all categories compared. Even without significance tests, it without doubt which approach is leading for the concept recommendation, at least, for our real-world dataset) and mapped the examples we observed to corresponding concept recommendations.\n",
    "E.g., when an EClass with name *User* is added via a containment to an EClass with the name *Software*, we used the tuple [EClass.Software, EClass.User] in Chaaben et al.'s approach. Depeding on the concrete ECore concept, we replaced *name* by the corresponding identifier (e.g., *key*).\n",
    "- How to sample context? => In their implementation, the authors sampled (random or all) pairs of concepts from the model at hand. Sampling all pairs is infeasible for real-world models (number of concepts squared concept pairs), and most of them would not be highly related. Since we had simple change graphs at hand, we decided to sample the edges/associations from these simple change graphs.\n",
    "- Which few-shot examples to choose? => This was one of the hardest choices. Basically this question can be considered even the major difference between the approaches. We could have decided to choose the few-shot samples from the dataset, similar to our approach. Anyway, since we consider this a crucial difference of the approach, we used the few-shot samples exactly as in the implementation of their approach. The few-shot samples were loaded from a file that we used in the re-implementation of their approach.\n",
    "- How to choose the recommendation? => First of all, it is noteworthy that several recommender systems make a list of k recommendations. We did intentionally decide to set k equal to one, since in a real world scenario (compare to GitHub Copilot), one would typically not come up with a list, but directly integrate one recommendation in the IDE. Anyway, we are also open to consider k>1 in future benchmarks if the community decides that this is necessary. In this case, in our evaluation (similar to Chaaben et al.'s work), we only considered the highes ranked concept. Using the approach as described and implemented by Chaaben et al. this would typicall recommend a concept that is already present in the model. Of course, we excluded these already present concepts and only ranked the list with new concepts and recommended the highest ranked concepet.\n",
    "- What language model to use? => We used exactly the same language model that has been used in the original implementation (text-davinci-002). We thought about re-implementing their appraoch on the basis of GPT-4. Anyway, this would lead to other necessary design choices. We used their prompts with GPT-4, and, as one would expect, the models starts generating natural language text not related to the concrete use case. This is because text-davinci-002 was not a *chat-like* model, while ChatGPT and GPT-4 are. In consequence, this would require a redsign of the prompts. \n",
    "- What metrics to use in the comparison? => To get a clearer picture of the pros and cons of the approaches, we decided to report independently the accuracy of the correct concepts being recommended, the accuracy of correct association being recommended, and we further split in correct type (e.g., EClass in the example above) and correct name (e.g., Software, or User in the example above).\n",
    "\n",
    "\n",
    "We highly acknowledge and appreciate the work of Chaaben et al. We don't believe that it was intended by the authors to be treated as a baseline for a model completion approach, as indicated by their paper naming *Towards...*. \n",
    "Anyway, we were required to deliver a baseline, and Chaaben et al.'s approach was the only one were still a glimpse of feasibility of a comparison could be recognized by the authors of this work.\n",
    "The differences of the approaches are the language model used (GPT-4 in our case and text-davinci-002 in theirs), the strategy of how the few-shot samples are retrieved, and the representation of the models, concepts, attributes, and relationships in the context of the prompt. \n",
    "We are certain that the most important feature is the RAG with semantic retrieval. As we have described in the paper, we retrieved very similar few-shot samples, which of course, help the language model to perform a completion.\n",
    "The language model has a smaller influence as we know from other experiment. A chat-like prompt and chat-model would improve the results of Chaaben et al. approach, though the semantic retrieval is more curical to the differences observed in the comparison.\n",
    "Even less, the concrete representation of model and model changes has an influence on the results of model completion. Anyway, there are several dimensions and not all of them can be investigated in this paper and are therefore left as future research.\n",
    "\n",
    "To better understand the individual influence of several design choice (or combinations theirof) for model completion, we highly encourage the community (around intelligent modeling assistants and for model completion, in particular) to assure that a common dataset (e.g., on the basis of the revision dataset, which includes histories) is evolved, that we work on a common set of reportings, independent of a concrete application domain (such as Simulink, BPM, SysML, or UML) and approach (e.g., using concepts such as edit operations in the evaluation). \n",
    "In the broader field of intelligent modeling assistants, we have to work on a common infrastruction to perform research and which enables us to investigate the pros and cons of the designs and design decisions proposed for the use cases or problem settings in the field.\n",
    "\n",
    "## Threats to validity\n",
    "We see a large homogenity in the data (a lot of EStringToStringMapEntries with key documentation)\n",
    "\n",
    "## Differences between the original approach and our implementation and evaluation\n",
    "- (-) We do not apply the approach to UML classdiagram\n",
    "- (+) We apply the approach to Ecore diagrams in a way that is compatible to class diagrams (for add concept, add relationship comparison). Add relationship we evaluate many-fold. First, in Ecore add \"EReference\" can be interpreted, roughly speaking, to adding associations in class diagrams. There are some occurrences of this in our ``add concept'' evlaution. Furthermore, we can evaluate if the correct edge (source, target nodes, directions, and association name), is correct in scenarios where a Reference is added in Ecore.\n",
    "- (-) We do not use a large random sample and rank\n",
    "- (+) We use the simple change graph, wich already focuses on the changing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =[pd.read_csv(file) for file in DATASET_RESULTS_FILES]\n",
    "\n",
    "columns = ['dataset', 'same_class', 'same_name', 'same_concept',  'same_association'] #'same_anchor_node',\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adapt the files a little bit for the unified comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use id as index for chaaben dataset\n",
    "datasets[2].set_index('id', inplace=True)\n",
    "\n",
    "# dataset index given by the following constraint (these are the only ones for which we can perform the comparison with Chaaben et al. )\n",
    "is_add_object = datasets[1]['completion'].str.contains(\"'changeType': 'Add', 'type': 'object'\")\n",
    "\n",
    "index = is_add_object[is_add_object].index\n",
    "\n",
    "# we compute same_concept = same_class==True and same_name==True\n",
    "for i, dataset in enumerate(datasets):\n",
    "    dataset['same_concept'] = dataset['same_class'] & dataset['same_name']\n",
    "    \n",
    "# same association is just a renaming of 'simplified_correct'\n",
    "for i, dataset in enumerate(datasets):\n",
    "    if 'simplified_correct' in dataset.columns and not 'same_association' in dataset.columns:\n",
    "        dataset['same_association'] = dataset['simplified_correct']\n",
    "\n",
    "# restrict all datasets to the index\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset.loc[index]\n",
    "    \n",
    "# only keep the following columns  \n",
    "keep_columns = ['same_class', 'same_name', 'same_concept', 'same_association']\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset[keep_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stastical evaluation\n",
    "We compute the correctness (probability of correct completion) for each class. We furthermore compare statistical significance (p-value), w.r.t. to 'random' via binomial test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import binomial test from scipy\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "# same class\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    # compute all probabilities\n",
    "    results_df.loc[i, columns[0]] = dataset_name[i] # set the dataset name\n",
    "    results_df.loc[i+len(datasets), columns[0]] = dataset_name[i] + \"_p\" # set the dataset name\n",
    "    for column in columns[1:]:\n",
    "        p = dataset[column].sum()/len(dataset)\n",
    "        results_df.loc[i, column] = p    \n",
    "        # compute binomial test with random baseline = dataset\n",
    "        # alternative\n",
    "        if dataset_name[i] == 'random':\n",
    "            alternative = 'two-sided' # actually we compare to the random, so this will be p-value = 1.0 anyways\n",
    "        elif dataset_name[i] == 'ramc':\n",
    "            alternative = 'greater' # we assume that semantical retrieval outperforms random retrieval\n",
    "        elif dataset_name[i] == 'chaaben':\n",
    "            alternative = 'less' # our hypothesis is that our random approach is better because it takes into account more similar examples via retrieval of few-shots\n",
    "        p_values = binom_test(x=dataset[column].sum(), n=len(dataset), p=datasets[0][column].sum()/len(dataset), alternative=alternative)\n",
    "        results_df.loc[i+len(datasets), column] = p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results_df formatted\n",
    "latex_table = results_df.to_latex(index=False, float_format=\"%.40f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "dataset & same_class & same_name & same_concept & same_association \\\\\n",
      "\\midrule\n",
      "random & 0.7843137254901960675468330919102299958467 & 0.8039215686274510108688673426513560116291 & 0.7647058823529411242247988411691039800644 & 0.6862745098039215729812667632359080016613 \\\\\n",
      "random_p & 1.0000000000000000000000000000000000000000 & 1.0000000000000000000000000000000000000000 & 1.0000000000000000000000000000000000000000 & 1.0000000000000000000000000000000000000000 \\\\\n",
      "ramc & 0.9411764705882352810561997102922759950161 & 0.9607843137254902243782339610334020107985 & 0.9411764705882352810561997102922759950161 & 0.8039215686274510108688673426513560116291 \\\\\n",
      "ramc_p & 0.0022647058234236570536945798437500343425 & 0.0013082316428690038846022192231544067909 & 0.0008507790945998238178391015473778224987 & 0.0443310839736497722118890862930129515007 \\\\\n",
      "chaaben & 0.2156862745098039324531669080897700041533 & 0.0980392156862745084433541364887787494808 & 0.0980392156862745084433541364887787494808 & 0.0784313725490196067546833091910229995847 \\\\\n",
      "chaaben_p & 0.0000000000000000080004675240996987169855 & 0.0000000000000000000000000022913209167288 & 0.0000000000000000000000078853898483616374 & 0.0000000000000000001253977921801001388710 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "dataset & same_class & same_name & same_concept &  same_association \\\n",
      "\\midrule\n",
      "random & 0.784 & 0.804 & 0.765 & 0.686 \\\n",
      "ramc & 0.941** & 0.961** & 0.941** & 0.804* \\\n",
      "chaaben & 0.216** & 0.098** & 0.098** & 0.078**\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \\begin{tabular}{lllll}\n",
    "# \\toprule\n",
    "# dataset & same_class & same_name & same_concept & same_association \\\\\n",
    "# \\midrule\n",
    "# random & 0.784 & 0.804 & 0.765 & 0.686 \\\\\n",
    "# random_p & 1.000 & 1.000 & 1.000 & 1.000 \\\\\n",
    "# ramc & 0.941 & 0.961 & 0.941 & 0.804 \\\\\n",
    "# ramc_p & 0.002 & 0.001 & 0.001 & 0.044 \\\\\n",
    "# chaaben & 0.216 & 0.098 & 0.098 & 0.078 \\\\\n",
    "# chaaben_p & 0.000 & 0.000 & 0.000 & 0.000 \\\\\n",
    "# \\bottomrule\n",
    "# \\end{tabular}\n",
    "\n",
    "\n",
    "# in the latex table above, add two stars if p-value < 0.01, one star if p-value < 0.05 for the corresponding approach, remove the columns with p-values\n",
    "latex_table_p = \"\"\"\n",
    "\\\\begin{tabular}{lllll}\n",
    "\\\\toprule\n",
    "dataset & same_class & same_name & same_concept &  same_association \\\\\n",
    "\\\\midrule\n",
    "random & 0.784 & 0.804 & 0.765 & 0.686 \\\\\n",
    "ramc & 0.941** & 0.961** & 0.941** & 0.804* \\\\\n",
    "chaaben & 0.216** & 0.098** & 0.098** & 0.078**\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\"\"\"\n",
    "print(latex_table_p)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
